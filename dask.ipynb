{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa715f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.10 | packaged by Anaconda, Inc. | (main, Oct  3 2024, 07:22:26) [MSC v.1929 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c74e4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "\n",
    "#import dask\n",
    "#print(dask.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb026a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dask[dataframe] in c:\\users\\rowida\\anaconda3\\anacondaa\\lib\\site-packages (2022.7.0)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in c:\\users\\rowida\\anaconda3\\anacondaa\\lib\\site-packages (from dask[dataframe]) (2024.6.1)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\rowida\\anaconda3\\anacondaa\\lib\\site-packages (from dask[dataframe]) (6.0)\n",
      "Requirement already satisfied: cloudpickle>=1.1.1 in c:\\users\\rowida\\anaconda3\\anacondaa\\lib\\site-packages (from dask[dataframe]) (2.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rowida\\anaconda3\\anacondaa\\lib\\site-packages (from dask[dataframe]) (22.0)\n",
      "Requirement already satisfied: partd>=0.3.10 in c:\\users\\rowida\\anaconda3\\anacondaa\\lib\\site-packages (from dask[dataframe]) (1.2.0)\n",
      "Requirement already satisfied: toolz>=0.8.2 in c:\\users\\rowida\\anaconda3\\anacondaa\\lib\\site-packages (from dask[dataframe]) (0.12.0)\n",
      "Requirement already satisfied: pandas>=1.0 in c:\\users\\rowida\\anaconda3\\anacondaa\\lib\\site-packages (from dask[dataframe]) (1.5.3)\n",
      "Requirement already satisfied: numpy>=1.18 in c:\\users\\rowida\\anaconda3\\anacondaa\\lib\\site-packages (from dask[dataframe]) (1.23.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\rowida\\anaconda3\\anacondaa\\lib\\site-packages (from pandas>=1.0->dask[dataframe]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rowida\\anaconda3\\anacondaa\\lib\\site-packages (from pandas>=1.0->dask[dataframe]) (2022.7)\n",
      "Requirement already satisfied: locket in c:\\users\\rowida\\anaconda3\\anacondaa\\lib\\site-packages (from partd>=0.3.10->dask[dataframe]) (1.0.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rowida\\anaconda3\\anacondaa\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=1.0->dask[dataframe]) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install dask[dataframe]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "018d2e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dask.dataframe as dd\n",
    "import time\n",
    "#from dask.distributed import Client\n",
    "#client = Client()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9c08ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a large pandas DataFrame and save it as a CSV file\n",
    "df = pd.DataFrame({\n",
    "    'A': range(1, 1000001),  # 1 million rows\n",
    "    'B': range(1000000, 0, -1),\n",
    "    'C': range(500000, 1500000)\n",
    "})\n",
    "\n",
    "df1=pd.DataFrame({\n",
    "    'A': range(1, 1000001),  # 1 million rows\n",
    "    'B': range(1000000, 0, -1),\n",
    "    'C': range(500000, 1500000)\n",
    "})\n",
    "#df.to_csv('large_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15bf1af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A        B       C\n",
      "0  1  1000000  500000\n",
      "1  2   999999  500001\n",
      "2  3   999998  500002\n",
      "3  4   999997  500003\n",
      "4  5   999996  500004\n",
      "\n",
      "              A  B        C\n",
      "999995   999996  5  1499995\n",
      "999996   999997  4  1499996\n",
      "999997   999998  3  1499997\n",
      "999998   999999  2  1499998\n",
      "999999  1000000  1  1499999\n",
      "\n",
      "3\n",
      "A    int64\n",
      "B    int64\n",
      "C    int64\n",
      "dtype: object\n",
      "<class 'dask_expr.DataFrame'>\n",
      "Columns: 3 entries, A to C\n",
      "dtypes: int64(3)None\n"
     ]
    }
   ],
   "source": [
    "#import dask.dataframe as dd\n",
    "#df = dd.read_csv('large_dataset.csv')\n",
    "#Each partition represents a piece of the data.\n",
    "dff = dd.from_pandas(df,npartitions=3)\n",
    "#computes the DataFrame and stores it in memory (RAM),optimize performance.\n",
    "dff = dff.persist()\n",
    "# Preview the data\n",
    "print(dff.head())\n",
    "print(\"\")\n",
    "print(dff.tail())\n",
    "print(\"\")\n",
    "print(dff.npartitions)\n",
    "print(dff.dtypes)\n",
    "print(dff.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84ad901e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows: 1000000\n",
      "Number of Columns: 3\n",
      "Columns of Dask DataFrame: ['A', 'B', 'C']\n"
     ]
    }
   ],
   "source": [
    "shape = dff.shape  # Returns a Dask array representing the shape\n",
    "\n",
    "# Compute the number of rows\n",
    "num_rows = dff.shape[0].compute()  # Number of rows\n",
    "\n",
    "# Get the number of columns directly\n",
    "num_columns = dff.shape[1]  # Number of columns (no compute needed)\n",
    "\n",
    "# Get the columns of the Dask DataFrame\n",
    "columns_result = dff.columns.tolist()  # Convert columns to list\n",
    "\n",
    "print(\"Number of Rows:\", num_rows)\n",
    "print(\"Number of Columns:\", num_columns)\n",
    "print(\"Columns of Dask DataFrame:\", columns_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b4dfd8",
   "metadata": {},
   "source": [
    "# loc,iloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2af77837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       A        B       C\n",
      "0      1  1000000  500000\n",
      "1      2   999999  500001\n",
      "2      3   999998  500002\n",
      "3      4   999997  500003\n",
      "4      5   999996  500004\n",
      "..   ...      ...     ...\n",
      "96    97   999904  500096\n",
      "97    98   999903  500097\n",
      "98    99   999902  500098\n",
      "99   100   999901  500099\n",
      "100  101   999900  500100\n",
      "\n",
      "[101 rows x 3 columns]\n",
      "              C        A        B\n",
      "0        500000        1  1000000\n",
      "1        500001        2   999999\n",
      "2        500002        3   999998\n",
      "3        500003        4   999997\n",
      "4        500004        5   999996\n",
      "...         ...      ...      ...\n",
      "999995  1499995   999996        5\n",
      "999996  1499996   999997        4\n",
      "999997  1499997   999998        3\n",
      "999998  1499998   999999        2\n",
      "999999  1499999  1000000        1\n",
      "\n",
      "[1000000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#loc[]: Selects rows by label and includes the endpoint.\n",
    "#iloc[]: Selects rows by integer position and excludes the endpoint. \n",
    "print(dff.loc[0:100].compute())   \n",
    "\n",
    "#selected_rows_iloc = dff.iloc[0:100, :].compute()\n",
    "#print(selected_rows_iloc)\n",
    "\n",
    "#Only indexing the column positions is supported. Trying to select row positions will raise a ValueError.\n",
    "ss=df.iloc[:, [2, 0, 1]]\n",
    "print(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dae09fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             A        B       C\n",
      "0            1  1000000  500000\n",
      "1            2   999999  500001\n",
      "2            3   999998  500002\n",
      "3            4   999997  500003\n",
      "4            5   999996  500004\n",
      "...        ...      ...     ...\n",
      "333329  333330   666671  833329\n",
      "333330  333331   666670  833330\n",
      "333331  333332   666669  833331\n",
      "333332  333333   666668  833332\n",
      "333333  333334   666667  833333\n",
      "\n",
      "[333334 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#Without .compute(), Dask only builds a task graph and won't show the actual data.\n",
    "#access a particular partition\n",
    "print(dff.partitions[0].compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc61c91",
   "metadata": {},
   "source": [
    "# select columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0da9b62d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0               1\n",
      "1               2\n",
      "2               3\n",
      "3               4\n",
      "4               5\n",
      "           ...   \n",
      "999995     999996\n",
      "999996     999997\n",
      "999997     999998\n",
      "999998     999999\n",
      "999999    1000000\n",
      "Name: A, Length: 1000000, dtype: int64\n",
      "              C        B\n",
      "0        500000  1000000\n",
      "1        500001   999999\n",
      "2        500002   999998\n",
      "3        500003   999997\n",
      "4        500004   999996\n",
      "...         ...      ...\n",
      "999995  1499995        5\n",
      "999996  1499996        4\n",
      "999997  1499997        3\n",
      "999998  1499998        2\n",
      "999999  1499999        1\n",
      "\n",
      "[1000000 rows x 2 columns]\n",
      "             A       B        C\n",
      "500000  500001  500000  1000000\n",
      "500001  500002  499999  1000001\n",
      "500002  500003  499998  1000002\n",
      "500003  500004  499997  1000003\n",
      "500004  500005  499996  1000004\n"
     ]
    }
   ],
   "source": [
    "# Select a column\n",
    "print(dff['A'].compute())\n",
    "print(dff[[\"C\",\"B\"]].compute())\n",
    "\n",
    "# Filter rows where 'A' is greater than 500,000\n",
    "filtered_df = dff[dff['A'] > 500000].compute()\n",
    "print(filtered_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07db8aed",
   "metadata": {},
   "source": [
    "# groupby & aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fdfef7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "1           500000.0\n",
      "2           500001.0\n",
      "3           500002.0\n",
      "4           500003.0\n",
      "5           500004.0\n",
      "             ...    \n",
      "999996     1499995.0\n",
      "999997     1499996.0\n",
      "999998     1499997.0\n",
      "999999     1499998.0\n",
      "1000000    1499999.0\n",
      "Name: C, Length: 1000000, dtype: float64\n",
      "\n",
      "             sum       mean      min      max\n",
      "A                                            \n",
      "1        1000000  1000000.0  1000000  1000000\n",
      "2         999999   999999.0   999999   999999\n",
      "3         999998   999998.0   999998   999998\n",
      "4         999997   999997.0   999997   999997\n",
      "5         999996   999996.0   999996   999996\n",
      "...          ...        ...      ...      ...\n",
      "999996         5        5.0        5        5\n",
      "999997         4        4.0        4        4\n",
      "999998         3        3.0        3        3\n",
      "999999         2        2.0        2        2\n",
      "1000000        1        1.0        1        1\n",
      "\n",
      "[1000000 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "#same as in pandas\n",
    "print(dff.groupby(\"A\")[\"C\"].mean().compute()) #same values of c\n",
    "print(\"\")\n",
    "#GroupBy with Multiple Aggregations\n",
    "print(dff.groupby(\"A\")['B'].agg(['sum', 'mean', 'min', 'max']).compute()) #same values of b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a236442a",
   "metadata": {},
   "source": [
    "# create new column and apply sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2183d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              A        B        C       A_sqrt\n",
      "0             1  1000000   500000     1.000000\n",
      "1             2   999999   500001     1.414214\n",
      "2             3   999998   500002     1.732051\n",
      "3             4   999997   500003     2.000000\n",
      "4             5   999996   500004     2.236068\n",
      "...         ...      ...      ...          ...\n",
      "999995   999996        5  1499995   999.998000\n",
      "999996   999997        4  1499996   999.998500\n",
      "999997   999998        3  1499997   999.999000\n",
      "999998   999999        2  1499998   999.999500\n",
      "999999  1000000        1  1499999  1000.000000\n",
      "\n",
      "[1000000 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "#use apply to create new column or apply function to each row or column elemnet\n",
    "import math\n",
    "dff['A_sqrt'] = dff['A'].apply(math.sqrt, meta=('A_sqrt', 'f8'))\n",
    "result=dff.compute()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3fde0a",
   "metadata": {},
   "source": [
    "# Statistical Operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c98a8da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000.5\n",
      "1\n",
      "1000000\n",
      "500000500000\n"
     ]
    }
   ],
   "source": [
    "#tha same in pandas\n",
    "print(dff[\"A\"].mean().compute())\n",
    "print(dff[\"A\"].min().compute())\n",
    "print(dff[\"A\"].max().compute())\n",
    "print(dff[\"A\"].sum().compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e456f7fb",
   "metadata": {},
   "source": [
    "\n",
    "# concate dask along rows& columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4632f555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Concatenated Dask DataFrame (rows):\n",
      "               A        B        C    A_sqrt\n",
      "0             1  1000000   500000  1.000000\n",
      "1             2   999999   500001  1.414214\n",
      "2             3   999998   500002  1.732051\n",
      "3             4   999997   500003  2.000000\n",
      "4             5   999996   500004  2.236068\n",
      "...         ...      ...      ...       ...\n",
      "999995   999996        5  1499995       NaN\n",
      "999996   999997        4  1499996       NaN\n",
      "999997   999998        3  1499997       NaN\n",
      "999998   999999        2  1499998       NaN\n",
      "999999  1000000        1  1499999       NaN\n",
      "\n",
      "[2000000 rows x 4 columns]\n",
      "\n",
      "Concatenated Dask DataFrame (columns):\n",
      "               A        B        C       A_sqrt        A        B        C\n",
      "0             1  1000000   500000     1.000000        1  1000000   500000\n",
      "1             2   999999   500001     1.414214        2   999999   500001\n",
      "2             3   999998   500002     1.732051        3   999998   500002\n",
      "3             4   999997   500003     2.000000        4   999997   500003\n",
      "4             5   999996   500004     2.236068        5   999996   500004\n",
      "...         ...      ...      ...          ...      ...      ...      ...\n",
      "999995   999996        5  1499995   999.998000   999996        5  1499995\n",
      "999996   999997        4  1499996   999.998500   999997        4  1499996\n",
      "999997   999998        3  1499997   999.999000   999998        3  1499997\n",
      "999998   999999        2  1499998   999.999500   999999        2  1499998\n",
      "999999  1000000        1  1499999  1000.000000  1000000        1  1499999\n",
      "\n",
      "[1000000 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "dff1= dd.from_pandas(df1,npartitions=3)\n",
    "concat_dff_rows = dd.concat([dff, dff1]) #along rows(vertically)\n",
    "print(\"\\nConcatenated Dask DataFrame (rows):\\n\", concat_dff_rows.compute())\n",
    "\n",
    "# Concatenate Dask DataFrames along columns\n",
    "concat_ddf_columns = dd.concat([dff, dff1], axis=1)\n",
    "print(\"\\nConcatenated Dask DataFrame (columns):\\n\", concat_ddf_columns.compute())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db715cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   k   l  rolling_mean\n",
      "0  0  50           NaN\n",
      "1  1  51           NaN\n",
      "2  2  52           1.0\n",
      "3  3  53           2.0\n",
      "4  4  54           3.0\n",
      "5  5  55           4.0\n",
      "6  6  56           5.0\n",
      "7  7  57           6.0\n",
      "8  8  58           7.0\n",
      "9  9  59           8.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df2 = pd.DataFrame({\n",
    "    'k': range(0, 50),  \n",
    "    'l': range(50, 100)  \n",
    "})\n",
    "dff2 = dd.from_pandas(df2, npartitions=2)\n",
    "\n",
    "#The first two rows have NaN for the rolling mean because there are not enough data points to fill the window size of 3.\n",
    "dff2[\"rolling_mean\"]=dff2[\"k\"].rolling(window=3).mean()\n",
    "output=dff2.compute()\n",
    "print(output.head(10))\n",
    "#print(dff2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cbf1aa",
   "metadata": {},
   "source": [
    "# set & reset index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "042c1b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    k  rolling_mean\n",
      "l                  \n",
      "50  0           NaN\n",
      "51  1           NaN\n",
      "52  2           1.0\n",
      "53  3           2.0\n",
      "54  4           3.0\n",
      "55  5           4.0\n",
      "56  6           5.0\n",
      "57  7           6.0\n",
      "58  8           7.0\n",
      "59  9           8.0\n",
      "   index  k   l  rolling_mean\n",
      "0      0  0  50           NaN\n",
      "1      1  1  51           NaN\n",
      "2      2  2  52           1.0\n",
      "3      3  3  53           2.0\n",
      "4      4  4  54           3.0\n",
      "5      5  5  55           4.0\n",
      "6      6  6  56           5.0\n",
      "7      7  7  57           6.0\n",
      "8      8  8  58           7.0\n",
      "9      9  9  59           8.0\n"
     ]
    }
   ],
   "source": [
    "#set index # reset\n",
    "setindex_by_L=dff2.set_index(\"l\").compute()\n",
    "print(setindex_by_L.head(10))\n",
    "undo_index=dff2.reset_index().compute()\n",
    "print(undo_index.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c9c361",
   "metadata": {},
   "source": [
    "# Rolling,Cumulative sum dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01f10c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dask DataFrame:\n",
      "   k   l  rolling_mean  rolling_sum_dask  cumsum_dask\n",
      "0  0  50           NaN               NaN            0\n",
      "1  1  51           NaN               NaN            1\n",
      "2  2  52           1.0               3.0            3\n",
      "3  3  53           2.0               6.0            6\n",
      "4  4  54           3.0               9.0           10\n",
      "5  5  55           4.0              12.0           15\n",
      "6  6  56           5.0              15.0           21\n",
      "7  7  57           6.0              18.0           28\n",
      "8  8  58           7.0              21.0           36\n",
      "9  9  59           8.0              24.0           45\n"
     ]
    }
   ],
   "source": [
    "#Calculate the rolling sum of column 'k' with a window of 3\n",
    "dff2['rolling_sum_dask'] = dff2['k'].rolling(window=3).sum()\n",
    "\n",
    "#Cumulative sum (Dask),dask don't support expanding\n",
    "dff2['cumsum_dask'] = dff2['k'].cumsum()\n",
    "\n",
    "# Apply expanding sum\n",
    "#dff2['A_expanding_sum'] = dff2['k'].expanding().sum()\n",
    "\n",
    "result_dask = dff2.compute()\n",
    "print(\"\\nDask DataFrame:\")\n",
    "print(result_dask.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f035c9e1",
   "metadata": {},
   "source": [
    "# Rolling,Cumulative sum,expanding pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5c6eeb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas DataFrame:\n",
      "   k   l  rolling_sum_pandas  expanding_sum_pandas  cumsum_pandas\n",
      "0  0  50                 NaN                   0.0              0\n",
      "1  1  51                 NaN                   1.0              1\n",
      "2  2  52                 3.0                   3.0              3\n",
      "3  3  53                 6.0                   6.0              6\n",
      "4  4  54                 9.0                  10.0             10\n",
      "5  5  55                12.0                  15.0             15\n",
      "6  6  56                15.0                  21.0             21\n",
      "7  7  57                18.0                  28.0             28\n",
      "8  8  58                21.0                  36.0             36\n",
      "9  9  59                24.0                  45.0             45\n"
     ]
    }
   ],
   "source": [
    "#Rolling sum with a window size of 3\n",
    "df2['rolling_sum_pandas'] = df2['k'].rolling(window=3).sum() #rolling in pandas & dask give the same results\n",
    "\n",
    "#Expanding sum\n",
    "df2['expanding_sum_pandas'] = df2['k'].expanding().sum() #expanding & cumsum give the same results\n",
    "\n",
    "#Cumulative sum\n",
    "df2['cumsum_pandas'] = df2['k'].cumsum()\n",
    "\n",
    "print(\"Pandas DataFrame:\")\n",
    "print(df2.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cfa58163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   k   l  rolling_mean  rolling_sum_dask  cumsum_dask   new\n",
      "0  0  50           NaN               NaN            0  50.0\n",
      "1  1  51           NaN               NaN            1  51.0\n",
      "2  2  52           1.0               3.0            3  52.0\n",
      "3  3  53           2.0               6.0            6  53.0\n",
      "4  4  54           3.0               9.0           10  54.0\n",
      "5  5  55           4.0              12.0           15  55.0\n",
      "6  6  56           5.0              15.0           21  56.0\n",
      "7  7  57           6.0              18.0           28  57.0\n",
      "8  8  58           7.0              21.0           36  58.0\n",
      "9  9  59           8.0              24.0           45  59.0\n"
     ]
    }
   ],
   "source": [
    "#Convert Data Types:\n",
    "dff2['new'] = dff2['l'].astype('float64')\n",
    "output=dff2.compute()\n",
    "print(output.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861d2463",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37b0e402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           hello_column name_column          lower_column  substring_column\n",
      "0           Hello World      rowida           hello world             False\n",
      "1         Dask is great       taher         dask is great             False\n",
      "2  DataFrame operations        fcis  dataframe operations              True\n",
      "3     Substring Example          IS     substring example              True\n"
     ]
    }
   ],
   "source": [
    "df3= pd.DataFrame({\n",
    "    'hello_column': ['Hello World', 'Dask is great', 'DataFrame operations', 'Substring Example'],\n",
    "     'name_column':['rowida','taher','fcis','IS']\n",
    "})\n",
    "\n",
    "# Convert it to a Dask DataFrame with 2 partitions\n",
    "dff3 = dd.from_pandas(df3, npartitions=2)\n",
    "\n",
    "dff3['lower_column'] = dff3['hello_column'].str.lower()  # Convert to lowercase\n",
    "\n",
    "dff3['substring_column'] = dff3['name_column'].str.contains('IS',case=False)# Check for substring case=True by default (case sensitive)\n",
    "result=dff3.compute()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f780c5",
   "metadata": {},
   "source": [
    "# query() method is functionally the same in both Dask and pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ebaa9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask DataFrame:\n",
      "    column1  column2\n",
      "10       11       20\n",
      "11       12       21\n",
      "12       13       22\n",
      "13       14       23\n",
      "14       15       24\n",
      "Pandas DataFrame:\n",
      "    column1  column2\n",
      "10       11       20\n",
      "11       12       21\n",
      "12       13       22\n",
      "13       14       23\n",
      "14       15       24\n"
     ]
    }
   ],
   "source": [
    "df4 = pd.DataFrame({\n",
    "    'column1': range(1, 21),  # Values from 1 to 20\n",
    "    'column2': range(10, 30)   # Values from 10 to 29\n",
    "})\n",
    "dff4 = dd.from_pandas(df4, npartitions=2)\n",
    "#query method is more readible then boolean indexing to filter rows based on string expression\n",
    "result = dff4.query('column1 > 10 and column2 < 25')\n",
    "print(\"Dask DataFrame:\")\n",
    "print(result.compute())\n",
    "\n",
    "pandas_rs=df4.query('column1 > 10 and column2 < 25')\n",
    "print(\"Pandas DataFrame:\")\n",
    "print(pandas_rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00486561",
   "metadata": {},
   "source": [
    "# pivot table to ggregate and reshape data in a DataFrame by pivoting around one or more columns.\n",
    "The columns attribute in the pivot_table method must be a categorical column. If you try to use a column that is not categorical, you will encounter a ValueError.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd13062a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column2     X     Y\n",
      "column1            \n",
      "A        40.0  20.0\n",
      "B        30.0  60.0\n",
      "C        50.0  60.0\n"
     ]
    }
   ],
   "source": [
    "#pivot table Often used for summary reports (check sles performance)\n",
    "#Groupby: More flexible for data manipulation\n",
    "df5 = pd.DataFrame({\n",
    "    'column1': ['A', 'A', 'B', 'B', 'C', 'C', 'A', 'B'],\n",
    "    'column2': ['X', 'Y', 'X', 'Y', 'X', 'Y', 'X', 'Y'],\n",
    "    'column3': [10, 20, 30, 40, 50, 60, 70, 80]\n",
    "})\n",
    "dff5 = dd.from_pandas(df5, npartitions=2)\n",
    "\n",
    "# Convert 'column2' to categorical it's mandatory\n",
    "dff5['column2'] = dff5['column2'].astype('category').compute()\n",
    "#Create a pivot table using Dask\n",
    "pivot_table = dff5.pivot_table(index='column1', columns='column2', values='column3', aggfunc='mean')\n",
    "\n",
    "# Compute the result to get the output\n",
    "pivot_table_result = pivot_table.compute()\n",
    "\n",
    "# Print the result\n",
    "print(pivot_table_result)\n",
    "#rs=dff5.memory_usage(deep=True).compute()  # Check memory usage of each column\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daee0c3",
   "metadata": {},
   "source": [
    "# Parallel Computing with Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "12019ed1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m delayed  \u001b[38;5;66;03m# Importing delayed function directly\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#import time\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Create a Dask client\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m client \u001b[38;5;241m=\u001b[39m Client()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# The @delayed decorator is used in Dask to turn a normal Python function into a lazy function.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;129m@delayed\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minc\u001b[39m(x):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Client' is not defined"
     ]
    }
   ],
   "source": [
    "#it's a simple task so dask introduces overhead and takes much time than pandas\n",
    "#from dask.distributed import Client\n",
    "from dask import delayed  # Importing delayed function directly\n",
    "#import time\n",
    "\n",
    "# Create a Dask client\n",
    "client = Client()\n",
    "\n",
    "# The @delayed decorator is used in Dask to turn a normal Python function into a lazy function.\n",
    "@delayed\n",
    "def inc(x):\n",
    "    return x + 1\n",
    "\n",
    "@delayed\n",
    "def add(x, y):\n",
    "    return x + y\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "a = inc(4).compute()       \n",
    "b = inc(2)       \n",
    "c = add(a, b)    \n",
    "\n",
    "c = c.compute()  \n",
    "end_time = time.time()\n",
    "\n",
    "print(c)\n",
    "print(f\"Execution Time with Dask Client: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2bbd2ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "Execution Time with Pandas: 0.00037169456481933594 seconds\n"
     ]
    }
   ],
   "source": [
    "#pandas time\n",
    "def inc(x):\n",
    "    return x + 1\n",
    "\n",
    "def add(x, y):\n",
    "    return x + y\n",
    "\n",
    "start_time_pandas = time.time()\n",
    "\n",
    "a = inc(4)  \n",
    "b = inc(2) \n",
    "c = add(a, b)\n",
    "\n",
    "end_time_pandas = time.time()\n",
    "\n",
    "print(c) \n",
    "print(f\"Execution Time with Pandas: {end_time_pandas - start_time_pandas} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3903b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         A  B        C     A_sqrt\n",
      "999995 NaN  5  1499995   999.9980\n",
      "999996 NaN  4  1499996   999.9985\n",
      "999997 NaN  3  1499997   999.9990\n",
      "999998 NaN  2  1499998   999.9995\n",
      "999999 NaN  1  1499999  1000.0000\n",
      "\n",
      "A         500\n",
      "B           0\n",
      "C           0\n",
      "A_sqrt      0\n",
      "dtype: int64\n",
      "\n",
      "          A  B        C     A_sqrt\n",
      "999995  1.0  5  1499995   999.9980\n",
      "999996  1.0  4  1499996   999.9985\n",
      "999997  1.0  3  1499997   999.9990\n",
      "999998  1.0  2  1499998   999.9995\n",
      "999999  1.0  1  1499999  1000.0000\n"
     ]
    }
   ],
   "source": [
    "#handling missing values\n",
    "dff[\"A\"]=dff[\"A\"].mask(dff[\"A\"]> 999500, None)\n",
    "print(dff.tail())\n",
    "print(\"\")\n",
    "\n",
    "print(dff.isnull().sum().tail())\n",
    "print(\"\")\n",
    "\n",
    "print(dff.fillna(1).tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed45063",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_concat = dd.concat([df1, df2])\n",
    "#df = df.dropna()  # Drop rows with NaN values\n",
    "#df_merged = dd.merge(df1, df2, on='key', how='inner')\n",
    "#df_sorted = df.sort_values('A').compute()\n",
    "#df = df.repartition(npartitions=10)\n",
    "#df = df.repartition(partition_size=\"100MB\")    Repartition by data size\n",
    "\n",
    "#df.groupby('key').apply(custom_function).compute() #aggregation\n",
    "\n",
    "#pandas_df = df.compute()  # Convert the entire Dask DataFrame to a pandas DataFrame\n",
    "#dask_df = dd.from_pandas(pandas_df, npartitions=5) #convert pandas to dask\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
